#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹‰å–å›½å®¶ç»´åº¦ä¸‹è½½/æ”¶å…¥æ•°æ®ã€‚ä»¿ç…§ advertisementsï¼Œå¯å†™å…¥ countiesdata/{å¹´}/{å‘¨}/json ä¸ xlsxï¼›
æœªæŒ‡å®šå¹´/å‘¨æ—¶å†™å…¥ request/country_data/json ä¸ xlsxã€‚

ä½¿ç”¨ Sensor Tower APIï¼šGET /v1/{os}/sales_report_estimates
- è·¯å¾„ï¼š{os}/sales_report_estimatesï¼Œç”¨ unifiedï¼ˆä¸æ¥å£æ–‡æ¡£ä¸€è‡´ï¼‰
- å‚æ•°ï¼šapp_idsï¼ˆå¿…å¡«ï¼‰ã€start_dateã€end_dateã€date_granularityã€countriesï¼ˆALL=æ‰€æœ‰åœ°åŒºï¼‰
pipeline ä¼ å…¥ Unified ID ä½œä¸º app_idï¼›æŒ‡å®š --yearã€--week æ—¶å†™å…¥ countiesdata/{å¹´}/{å‘¨}/ã€‚
"""

import argparse
import json
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# ST API ç½‘ç»œä¸ç¨³å®šï¼šå¤±è´¥æ—¶å»¶è¿Ÿåé‡è¯•ï¼Œæœ€å¤šè¯·æ±‚ 3 æ¬¡
RETRY_MAX = 3
RETRY_DELAY_SEC = 3

# ä¿è¯å¯å¯¼å…¥ request.api_requestï¼ˆä»é¡¹ç›®æ ¹æˆ– request ç›®å½•è¿è¡Œå‡å¯ï¼‰
REQUEST_DIR = Path(__file__).resolve().parent
_root = REQUEST_DIR.parent
if str(_root) not in sys.path:
    sys.path.insert(0, str(_root))

from request.api_request import load_token, get_session, get, ensure_dir

BASE_DIR = REQUEST_DIR.parent
# é»˜è®¤å†™ request/country_dataï¼›æŒ‡å®š year/week æ—¶åœ¨ run() ä¸­æ”¹ä¸º countiesdata/{å¹´}/{å‘¨}/
OUTPUT_JSON_DIR = REQUEST_DIR / "country_data" / "json"
OUTPUT_XLSX_DIR = REQUEST_DIR / "country_data" / "xlsx"


def fetch_country_data_for_app(
    session,
    app_id,
    os_platform="unified",
    start_date=None,
    end_date=None,
    date_granularity="weekly",
    countries="ALL",
    data_model="DM_2025_Q2",
    debug=False,
):
    """
    è¯·æ±‚å•ä¸ª app çš„ä¸‹è½½/æ”¶å…¥ä¼°ç®—ï¼ˆæŒ‰å›½å®¶ã€æ—¥æœŸï¼‰ã€‚
    API: GET /v1/{os}/sales_report_estimates
    å‚æ•°ï¼šapp_idsï¼ˆå¿…å¡«ï¼‰ã€start_dateã€end_dateã€date_granularityã€countriesï¼ˆALL=æ‰€æœ‰åœ°åŒºï¼‰ã€data_modelã€‚
    """
    path = f"{os_platform}/sales_report_estimates"
    params = {
        "app_ids": app_id,
        "date_granularity": date_granularity,
        "data_model": data_model,
        "countries": countries,
    }
    if start_date:
        params["start_date"] = start_date
    if end_date:
        params["end_date"] = end_date
    resp = get(session, path, params=params)
    resp.raise_for_status()
    result = resp.json()
    if debug:
        print(f"  [DEBUG] API è¿”å›ç±»å‹: {type(result)}")
        if isinstance(result, dict):
            print(f"  [DEBUG] è¿”å›å­—å…¸çš„ keys: {list(result.keys())}")
            if "data" in result:
                print(f"  [DEBUG] data é•¿åº¦: {len(result['data']) if isinstance(result['data'], (list, dict)) else 'N/A'}")
        elif isinstance(result, list):
            print(f"  [DEBUG] è¿”å›åˆ—è¡¨é•¿åº¦: {len(result)}")
            if result:
                print(f"  [DEBUG] ç¬¬ä¸€æ¡ keys: {list(result[0].keys()) if isinstance(result[0], dict) else 'N/A'}")
    return result


def fetch_country_data_with_retry(
    session,
    app_id,
    os_platform="unified",
    start_date=None,
    end_date=None,
    date_granularity="weekly",
    countries="ALL",
    data_model="DM_2025_Q2",
    debug=False,
):
    """å¸¦é‡è¯•çš„è¯·æ±‚ï¼šå¤±è´¥æ—¶å»¶è¿Ÿ RETRY_DELAY_SEC ç§’åé‡è¯•ï¼Œæœ€å¤š RETRY_MAX æ¬¡ã€‚"""
    last_e = None
    for attempt in range(RETRY_MAX):
        try:
            return fetch_country_data_for_app(
                session,
                app_id,
                os_platform=os_platform,
                start_date=start_date,
                end_date=end_date,
                date_granularity=date_granularity,
                countries=countries,
                data_model=data_model,
                debug=debug,
            )
        except Exception as e:
            last_e = e
            if attempt < RETRY_MAX - 1:
                time.sleep(RETRY_DELAY_SEC)
    raise last_e


def save_json(app_id, data, json_dir=None):
    d = json_dir or OUTPUT_JSON_DIR
    ensure_dir(d)
    path = d / f"{app_id}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  å·²å†™ JSON: {path}")


# åˆ†åœ°åŒºæ•°æ®è¡¨åˆ—é¡ºåºï¼ˆä¸å¸¸è§è¡¨æ ¼ä¸€è‡´ï¼šapp_id, country, date, android, iphone, unified, ipadï¼‰
COUNTRY_TABLE_COLUMN_ORDER = [
    "app_id",
    "country",
    "date",
    "android_units",
    "android_revenue",
    "iphone_units",
    "iphone_revenue",
    "unified_units",
    "unified_revenue",
    "ipad_units",
    "ipad_revenue",
]


def _format_date_for_excel(val):
    """å°† ISO æ—¥æœŸè½¬ä¸º YYYY-MM-DD å­—ç¬¦ä¸²ï¼Œä¾¿äº Excel æ˜¾ç¤ºï¼ˆé¿å… ######ï¼‰ã€‚"""
    if val is None or (isinstance(val, float) and pd.isna(val)):
        return ""
    s = str(val).strip()
    if not s:
        return ""
    # 2026-01-19T00:00:00Z -> 2026-01-19
    if "T" in s:
        s = s.split("T")[0]
    if len(s) >= 10 and s[4] == "-" and s[7] == "-":
        return s[:10]
    return s


def save_xlsx(app_id, data, xlsx_dir=None):
    """å°† list[dict] æ‹†æˆè¡¨æ ¼å†™å…¥ xlsxï¼šå›ºå®šåˆ—é¡ºåºã€æ—¥æœŸæ ¼å¼åŒ–ä¸º YYYY-MM-DDã€‚"""
    if not data or not isinstance(data, list):
        return
    try:
        import pandas as pd
        d = xlsx_dir or OUTPUT_XLSX_DIR
        ensure_dir(d)
        df = pd.DataFrame(data)
        # æ—¥æœŸåˆ—æ ¼å¼åŒ–ä¸º YYYY-MM-DDï¼Œé¿å… Excel æ˜¾ç¤º ######
        if "date" in df.columns:
            df["date"] = df["date"].apply(_format_date_for_excel)
        # æŒ‰çº¦å®šåˆ—é¡ºåºæ’è¡¨ï¼ˆå­˜åœ¨çš„åˆ—ä¼˜å…ˆï¼Œå…¶ä½™åˆ—è¿½åŠ åœ¨åï¼‰
        ordered = [c for c in COUNTRY_TABLE_COLUMN_ORDER if c in df.columns]
        rest = [c for c in df.columns if c not in ordered]
        df = df[ordered + rest]
        path = d / f"{app_id}.xlsx"
        df.to_excel(path, index=False)
        print(f"  å·²å†™ XLSX: {path}")
    except Exception as e:
        print(f"  å†™ XLSX è·³è¿‡: {e}")


# è·³è¿‡æ—¶æœ€å¤šæ‰“å°å‰å‡ æ¡è¯¦æƒ…ï¼Œé¿å…åˆ·å±
MAX_SKIP_LOGS = 3

# å¹¶å‘æ‹‰å–æ—¶æ¯ä¸ªçº¿ç¨‹æœ€å¤šåŒæ—¶è¯·æ±‚æ•°ï¼ˆé¿å…å‹å® API æˆ–è§¦å‘é™æµï¼‰
DEFAULT_CONCURRENCY = 1  # API æœ‰é™é¢ï¼Œä¸²è¡Œè¯·æ±‚


def _fetch_one(
    app_id,
    token,
    start_date,
    end_date,
    os_platform,
    date_granularity,
    countries,
    data_model,
    json_dir,
    xlsx_dir,
    save_xlsx_too,
    debug,
):
    """å•ä¸ª app æ‹‰å–å¹¶è½ç›˜ï¼Œä¾›çº¿ç¨‹æ± è°ƒç”¨ã€‚è¿”å› (app_id, None) æˆåŠŸï¼Œ(app_id, exception) å¤±è´¥ã€‚"""
    app_id = app_id.strip()
    if not app_id:
        return (app_id, None)
    try:
        session = get_session(token)
        if debug:
            print(f"  [è¯·æ±‚] app_id={app_id}, start_date={start_date}, end_date={end_date}, countries={countries}")
        data = fetch_country_data_with_retry(
            session,
            app_id,
            os_platform=os_platform,
            start_date=start_date,
            end_date=end_date,
            date_granularity=date_granularity,
            countries=countries,
            data_model=data_model,
            debug=debug,
        )
        if isinstance(data, dict) and "data" in data:
            data = data["data"]
        save_json(app_id, data, json_dir=json_dir)
        if save_xlsx_too:
            save_xlsx(app_id, data, xlsx_dir=xlsx_dir)
        return (app_id, None)
    except Exception as e:
        return (app_id, e)


def run(
    app_ids,
    start_date=None,
    end_date=None,
    os_platform="unified",
    date_granularity="weekly",
    countries="ALL",
    data_model="DM_2025_Q2",
    save_xlsx_too=False,
    strict=False,
    year=None,
    week_tag=None,
    product_type=None,
    debug=False,
    concurrency=None,
):
    """
    strict=Falseï¼šå•ä¸ª app è¯·æ±‚å¤±è´¥æ—¶è·³è¿‡å¹¶ç»§ç»­ï¼›strict=True æ—¶ä»»ä¸€å¤±è´¥å³æŠ›é”™ã€‚
    yearã€week_tagã€product_type è‹¥éƒ½æä¾›ï¼Œåˆ™å†™å…¥ countiesdata/{å¹´}/{å‘¨}/{product_type}/jsonï¼ˆé»˜è®¤ä¸å†™ xlsxï¼Œä»… json ä¾› build_final_join ä¸ MySQL åŒæ­¥ç”¨ï¼‰ã€‚
    concurrencyï¼šå¹¶å‘æ•°ï¼Œé»˜è®¤ 1ï¼ˆä¸²è¡Œï¼‰ï¼›è®¾ä¸º 1 åˆ™é€€åŒ–ä¸ºä¸²è¡Œã€‚
    """
    json_dir = OUTPUT_JSON_DIR
    xlsx_dir = OUTPUT_XLSX_DIR
    if year is not None and week_tag and product_type:
        json_dir = BASE_DIR / "countiesdata" / str(year) / week_tag / product_type / "json"
        xlsx_dir = BASE_DIR / "countiesdata" / str(year) / week_tag / product_type / "xlsx"
        print(f"  å†™å…¥: countiesdata/{year}/{week_tag}/{product_type}/json" + (" ä¸ xlsx" if save_xlsx_too else ""))
    token = load_token()
    workers = int(concurrency) if concurrency is not None else DEFAULT_CONCURRENCY
    workers = max(1, min(workers, 20))
    app_ids = [a.strip() for a in app_ids if a and a.strip()]
    ok, fail = 0, 0
    if workers <= 1:
        total = len(app_ids)
        for idx, app_id in enumerate(app_ids, 1):
            print(f"  ğŸ”¹ æ‹‰å–åœ°åŒºæ•°æ®ï¼š{idx}/{total} app_id={app_id}")
            _, exc = _fetch_one(
                app_id, token, start_date, end_date, os_platform,
                date_granularity, countries, data_model,
                json_dir, xlsx_dir, save_xlsx_too, debug,
            )
            if exc is None:
                ok += 1
            else:
                if fail < MAX_SKIP_LOGS:
                    print(f"  âš ï¸ è·³è¿‡ {app_id}: {exc}")
                fail += 1
                if strict:
                    raise exc
    else:
        print(f"  åœ°åŒºæ•°æ®å¹¶å‘æ•°: {workers}")
        for idx, app_id in enumerate(app_ids, 1):
            print(f"  ğŸ”¹ æ‹‰å–åœ°åŒºæ•°æ®ï¼š{idx}/{len(app_ids)} app_id={app_id}")
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(
                    _fetch_one,
                    app_id, token, start_date, end_date, os_platform,
                    date_granularity, countries, data_model,
                    json_dir, xlsx_dir, save_xlsx_too, debug,
                ): app_id
                for app_id in app_ids
            }
            for fut in as_completed(futures):
                app_id, exc = fut.result()
                if exc is None:
                    ok += 1
                else:
                    if fail < MAX_SKIP_LOGS:
                        print(f"  âš ï¸ è·³è¿‡ {app_id}: {exc}")
                    fail += 1
                    if strict:
                        raise exc
    if fail:
        if fail > MAX_SKIP_LOGS:
            print(f"  ... å…¶ä½™ {fail - MAX_SKIP_LOGS} ä¸ªå·²è·³è¿‡")
        print(f"  åœ°åŒºæ•°æ®ï¼šæˆåŠŸ {ok}ï¼Œè·³è¿‡ {fail}")


def main():
    parser = argparse.ArgumentParser(
        description="æ‹‰å–ä¸‹è½½/æ”¶å…¥ä¼°ç®—ï¼ˆAPI: GET /v1/{os}/sales_report_estimatesï¼‰"
    )
    parser.add_argument("--app_ids", nargs="+", help="ä¸€ä¸ªæˆ–å¤šä¸ª app_idï¼ˆUnified IDï¼‰")
    parser.add_argument("--app_ids_file", type=Path, help="æ¯è¡Œä¸€ä¸ª app_id çš„æ–‡ä»¶")
    parser.add_argument("--start_date", help="å¼€å§‹æ—¥æœŸ YYYY-MM-DDï¼ˆå¿…å¡«å»ºè®®ï¼‰")
    parser.add_argument("--end_date", help="ç»“æŸæ—¥æœŸ YYYY-MM-DDï¼ˆå¿…å¡«å»ºè®®ï¼‰")
    parser.add_argument("--os", dest="os_platform", default="unified", choices=["unified", "ios", "android"], help="å¹³å°ï¼Œé»˜è®¤ unifiedï¼ˆä¸æ¥å£ä¸€è‡´ï¼‰")
    parser.add_argument("--date_granularity", default="weekly", choices=["daily", "weekly", "monthly", "quarterly"], help="æ—¥æœŸç²’åº¦ï¼Œé»˜è®¤ weekly")
    parser.add_argument("--countries", default="ALL", help="å›½å®¶ï¼šALL=æ‰€æœ‰åœ°åŒºï¼Œæˆ– WW/é€—å·åˆ†éš”å›½å®¶ä»£ç ï¼Œé»˜è®¤ ALL")
    parser.add_argument("--data_model", default="DM_2025_Q2", choices=["DM_2025_Q1", "DM_2025_Q2"], help="æ•°æ®æ¨¡å‹ï¼Œé»˜è®¤ DM_2025_Q2")
    parser.add_argument("--xlsx", action="store_true", help="åŒæ—¶å†™å…¥ xlsxï¼ˆé»˜è®¤ä»…å†™ jsonï¼Œä¾› build_final_join ä¸ MySQL åŒæ­¥ï¼‰")
    parser.add_argument("--strict", action="store_true", help="ä»»ä¸€ app è¯·æ±‚å¤±è´¥å³é€€å‡ºï¼Œé»˜è®¤è·³è¿‡å¤±è´¥ç»§ç»­")
    parser.add_argument("--year", type=int, help="å¹´ä»½ï¼Œä¸ --weekã€--product_type ä¸€èµ·æŒ‡å®šæ—¶å†™å…¥ countiesdata/{å¹´}/{å‘¨}/{product_type}/")
    parser.add_argument("--week", dest="week_tag", help="å‘¨æ ‡ç­¾å¦‚ 1201-1207")
    parser.add_argument("--product_type", choices=["strategy_old", "strategy_new"], help="ä¸ --yearã€--week ä¸€èµ·æ—¶å†™å…¥ countiesdata/{å¹´}/{å‘¨}/strategy_old æˆ– strategy_new")
    parser.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, help=f"å¹¶å‘è¯·æ±‚æ•°ï¼Œé»˜è®¤ {DEFAULT_CONCURRENCY}ï¼›è®¾ä¸º 1 åˆ™ä¸²è¡Œ")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šåªè¯·æ±‚ç¬¬ä¸€ä¸ª app_idï¼Œæ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯")
    args = parser.parse_args()

    app_ids = []
    if args.app_ids:
        app_ids.extend(args.app_ids)
    if args.app_ids_file and args.app_ids_file.exists():
        app_ids.extend(args.app_ids_file.read_text(encoding="utf-8").strip().splitlines())
    if not app_ids:
        print("è¯·æä¾› --app_ids æˆ– --app_ids_file")
        sys.exit(1)
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†ç¬¬ä¸€ä¸ª app_id
    if args.test:
        app_ids = app_ids[:1]
        print(f"  [TEST] æµ‹è¯•æ¨¡å¼ï¼šåªè¯·æ±‚ç¬¬ä¸€ä¸ª app_id: {app_ids[0]}")

    run(
        app_ids,
        start_date=args.start_date,
        end_date=args.end_date,
        os_platform=args.os_platform,
        date_granularity=args.date_granularity,
        countries=args.countries,
        data_model=args.data_model,
        save_xlsx_too=args.xlsx,
        strict=args.strict,
        year=args.year,
        week_tag=args.week_tag,
        product_type=args.product_type,
        debug=args.test,
        concurrency=args.concurrency,
    )


if __name__ == "__main__":
    main()
