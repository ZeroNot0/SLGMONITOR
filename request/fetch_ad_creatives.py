#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹‰å–å¹¿å‘Šåˆ›æ„æ•°æ®ï¼ŒæŒ‰ app_id + åœ°åŒºå†™å…¥ advertisements/{å¹´}/{å‘¨}/ å¯¹åº”æ—¥æœŸæ–‡ä»¶å¤¹ä¸‹ã€‚

ä½¿ç”¨ Sensor Tower APIï¼šGET /v1/{os}/ad_intel/creatives
- è·¯å¾„ï¼š{os} ä¸º iosã€android æˆ– unified
- å‚æ•°ï¼šapp_idsï¼ˆå¿…å¡«ï¼Œé€—å·åˆ†éš”ï¼‰ã€start_dateï¼ˆå¿…å¡«ï¼‰ã€end_dateã€countriesï¼ˆå¿…å¡«ï¼Œå›½å®¶ä»£ç é€—å·åˆ†éš”ï¼‰
pipeline ä¼ å…¥ Unified ID ä½œä¸º app_idã€‚
"""

import argparse
import json
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# ST API ç½‘ç»œä¸ç¨³å®šï¼šå¤±è´¥æ—¶å»¶è¿Ÿåé‡è¯•ï¼Œæœ€å¤šè¯·æ±‚ 3 æ¬¡
RETRY_MAX = 3
RETRY_DELAY_SEC = 3

REQUEST_DIR = Path(__file__).resolve().parent
BASE_DIR = REQUEST_DIR.parent
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

from request.api_request import load_token, get_session, get, ensure_dir

try:
    from app.app_paths import get_data_root
    ADS_ROOT = get_data_root() / "advertisements"
except Exception:
    ADS_ROOT = BASE_DIR / "advertisements"

# æœªæŒ‡å®š --year/--week æ—¶çš„å…œåº•è¾“å‡ºï¼ˆrequest ä¸‹ï¼‰
FALLBACK_JSON_DIR = REQUEST_DIR / "ad_creatives" / "json"
FALLBACK_XLSX_DIR = REQUEST_DIR / "ad_creatives" / "xlsx"

# åœ°åŒºæ ‡ç­¾ï¼ˆä¸ mapping/å¸‚åœºTåº¦ ä¸€è‡´ï¼Œäºšæ´² T1 æ’é™¤ CNï¼Œæœªåœ¨è¡¨ä¸­çš„å›½å®¶å½’ T3ï¼‰
REGIONS = ["äºšæ´²T1", "æ¬§ç¾T1", "T2", "T3"]

# åˆ›æ„ APIï¼šåªè¦ video ç±»å‹
AD_TYPES_VIDEO = ["video"]

# åˆ›æ„ APIï¼šæ‰€æœ‰ç½‘ç»œï¼ˆä¸æ–‡æ¡£ä¸€è‡´ï¼‰
NETWORKS = [
    "Admob", "Applovin", "BidMachine", "Chartboost", "Digital Turbine",
    "Facebook", "InMobi", "Instagram", "Line", "Meta Audience Network",
    "Mintegral", "Moloco", "Pangle", "Pinterest", "Smaato", "Snapchat",
    "Supersonic", "TikTok", "Twitter", "Unity", "Verve", "Vungle", "Youtube",
]

# T3 å¸‚åœºå›½å®¶ä»£ç ï¼šå®Œæ•´ 50 å›½åˆ—è¡¨ä¸­æœªåœ¨ å¸‚åœºTåº¦.csvï¼ˆäºšæ´²T1/æ¬§ç¾T1/T2ï¼‰çš„å›½å®¶ï¼Œå…± 26 ä¸ª
T3_COUNTRIES = [
    "AR", "BR", "CL", "CN", "CO", "CZ", "EC", "GR", "ID", "IN", "LU",
    "MX", "MY", "NG", "PA", "PE", "PH", "PL", "PT", "RO", "RU", "TH",
    "TR", "UA", "VN", "ZA",
]


def week_tag_to_dates(year: int, week_tag: str):
    """å°† week_tagï¼ˆå¦‚ 0119-0125ï¼‰å’Œ year è½¬ä¸º start_dateã€end_dateï¼ˆYYYY-MM-DDï¼‰ã€‚è·¨å¹´å‘¨ï¼ˆå¦‚ 1229-0104ï¼‰æ—¶ end_date ç”¨ year+1ã€‚"""
    if not year or not week_tag:
        return None, None
    s = (week_tag or "").strip()
    m = re.match(r"^(\d{2})(\d{2})-(\d{2})(\d{2})$", s)
    if not m:
        return None, None
    m1, d1, m2, d2 = m.group(1), m.group(2), m.group(3), m.group(4)
    try:
        start_date = f"{year}-{m1}-{d1}"
        # è·¨å¹´å‘¨ï¼šå¦‚ 1229-0104ï¼Œç»“æŸæœˆåœ¨ 01 å°äºå¼€å§‹æœˆ 12ï¼Œåˆ™ end ç”¨ year+1
        end_year = year if int(m2) >= int(m1) else year + 1
        end_date = f"{end_year}-{m2}-{d2}"
        return start_date, end_date
    except Exception:
        return None, None


def load_market_countries():
    """
    ä» mapping/å¸‚åœºTåº¦.csv è¯»å– äºšæ´²T1 / æ¬§ç¾T1 / T2 å›½å®¶ä»£ç ï¼›
    T3 ä½¿ç”¨è„šæœ¬å†…å¸¸é‡ T3_COUNTRIESã€‚
    è¿”å› dict: {"äºšæ´²T1": "TW,HK,...", "æ¬§ç¾T1": "US,GB,...", "T2": "IT,ES,...", "T3": "BR,MX,..."}
    """
    csv_path = BASE_DIR / "mapping" / "å¸‚åœºTåº¦.csv"
    by_region = {r: [] for r in REGIONS}
    if csv_path.exists():
        import csv
        with open(csv_path, "r", encoding="utf-8") as f:
            for row in csv.DictReader(f):
                c = (row.get("country") or "").strip()
                t = (row.get("Tåº¦") or "").strip()
                if c and t and t in by_region:
                    by_region[t].append(c)
    by_region["T3"] = T3_COUNTRIES
    return {r: ",".join(by_region[r]) for r in REGIONS if by_region[r]}


def fetch_ad_creatives_for_app(
    session,
    app_id,
    os_platform="unified",
    start_date=None,
    end_date=None,
    countries="WW",
    ad_types=None,
    networks=None,
):
    """
    è¯·æ±‚å•ä¸ª app çš„å¹¿å‘Šåˆ›æ„æ•°æ®ã€‚
    API: GET /v1/{os}/ad_intel/creatives
    å‚æ•°ï¼šapp_idsã€start_dateã€end_dateã€countriesï¼ˆå¿…å¡«ï¼‰ã€ad_typesã€networksï¼ˆå¿…å¡«ï¼‰ã€‚
    """
    path = f"{os_platform}/ad_intel/creatives"
    params = {
        "app_ids": app_id,
        "countries": countries,
        "ad_types": ad_types or ",".join(AD_TYPES_VIDEO),
        "networks": networks or ",".join(NETWORKS),
    }
    if start_date:
        params["start_date"] = start_date
    if end_date:
        params["end_date"] = end_date
    resp = get(session, path, params=params)
    resp.raise_for_status()
    return resp.json()


def fetch_ad_creatives_with_retry(
    session,
    app_id,
    os_platform="unified",
    start_date=None,
    end_date=None,
    countries="WW",
    ad_types=None,
    networks=None,
):
    """å¸¦é‡è¯•çš„è¯·æ±‚ï¼šå¤±è´¥æ—¶å»¶è¿Ÿ RETRY_DELAY_SEC ç§’åé‡è¯•ï¼Œæœ€å¤š RETRY_MAX æ¬¡ã€‚"""
    last_e = None
    for attempt in range(RETRY_MAX):
        try:
            return fetch_ad_creatives_for_app(
                session,
                app_id,
                os_platform=os_platform,
                start_date=start_date,
                end_date=end_date,
                countries=countries,
                ad_types=ad_types,
                networks=networks,
            )
        except Exception as e:
            last_e = e
            if attempt < RETRY_MAX - 1:
                time.sleep(RETRY_DELAY_SEC)
    raise last_e


def _safe_folder_name(s):
    """æ–‡ä»¶å¤¹åä¸­é¿å… Windows éæ³•å­—ç¬¦ã€‚"""
    return re.sub(r'[<>:"/\\|?*]', "_", (s or "").strip()) or "unknown"


def get_output_dirs(year, week_tag, product_type, app_id, product_name):
    """
    è¿”å› (json_dir, xlsx_dir) ç”¨äº advertisements ä¸‹çš„æ—¥æœŸæ–‡ä»¶å¤¹ã€‚
    è·¯å¾„ï¼šadvertisements/{year}/{week_tag}/{product_type}/{app_id}_{product_name}/json ä¸ xlsx
    """
    folder = f"{app_id}_{_safe_folder_name(product_name)}"
    base = ADS_ROOT / str(year) / week_tag / product_type / folder
    return base / "json", base / "xlsx"


def save_json(app_id, data, json_dir=None, suffix=""):
    """suffix ä¸ºç©ºæ—¶ä¿å­˜ä¸º {app_id}.jsonï¼Œå¦åˆ™ä¸º {app_id}_{suffix}.json"""
    json_dir = json_dir or FALLBACK_JSON_DIR
    ensure_dir(json_dir)
    name = f"{app_id}_{suffix}.json" if suffix else f"{app_id}.json"
    path = json_dir / name
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  å·²å†™ JSON: {path}")


def save_xlsx(app_id, data, xlsx_dir=None, product_name=None, suffix=""):
    """è‹¥ data ä¸º dict ä¸”å«å¯è¡¨æ ¼åŒ–å†…å®¹ï¼Œåˆ™å¯¼å‡º xlsxã€‚suffix ç”¨äºæ–‡ä»¶ååŒºåˆ†ï¼ˆå¦‚ countriesï¼‰ã€‚"""
    try:
        import pandas as pd
        if not isinstance(data, dict):
            return
        ad_units = data.get("ad_units") or data.get("creatives") or []
        if not ad_units:
            return
        xlsx_dir = xlsx_dir or FALLBACK_XLSX_DIR
        ensure_dir(xlsx_dir)
        df = pd.DataFrame(ad_units)
        name = _safe_folder_name(product_name) if product_name else app_id
        base = f"{app_id}_{name}"
        path = xlsx_dir / f"{base}_{suffix}.xlsx" if suffix else f"{base}.xlsx"
        df.to_excel(path, index=False)
        print(f"  å·²å†™ XLSX: {path}")
    except Exception as e:
        print(f"  å†™ XLSX è·³è¿‡: {e}")


# è·³è¿‡æ—¶æœ€å¤šæ‰“å°å‰å‡ æ¡è¯¦æƒ…ï¼Œé¿å…åˆ·å±
MAX_SKIP_LOGS = 3

# åˆ›æ„æ•°æ®å¹¶å‘æ•°ï¼ˆæ¯äº§å“ 4 ä¸ªå¸‚åœºï¼Œå¹¶å‘å¯æ˜¾è‘—ç¼©çŸ­æ€»è€—æ—¶ï¼‰
DEFAULT_CONCURRENCY = 1  # API æœ‰é™é¢ï¼Œä¸²è¡Œè¯·æ±‚


def _fetch_one_region(
    app_id,
    product_name,
    region,
    token,
    start_date,
    end_date,
    os_platform,
    market_countries,
    use_advertisements,
    year,
    week_tag,
    product_type,
    save_xlsx_too,
):
    """å•æ¬¡ (app_id, region) æ‹‰å–å¹¶è½ç›˜ï¼Œä¾›çº¿ç¨‹æ± è°ƒç”¨ã€‚è¿”å› (app_id, region, None) æˆåŠŸï¼Œ(app_id, region, exception) å¤±è´¥ã€‚"""
    app_id = (app_id or "").strip()
    if not app_id:
        return (app_id, region, None)
    pname = (product_name or app_id).strip() or app_id
    region_countries = market_countries.get(region)
    if not region_countries:
        return (app_id, region, None)
    json_dir = xlsx_dir = None
    if use_advertisements and year and week_tag:
        json_dir, xlsx_dir = get_output_dirs(year, week_tag, product_type, app_id, pname)
    try:
        print(f"  ğŸ”¹ æ‹‰å–åˆ›æ„æ•°æ®ï¼šapp_id={app_id} äº§å“={pname} åœ°åŒº={region}")
        session = get_session(token)
        data = fetch_ad_creatives_with_retry(
            session,
            app_id,
            os_platform=os_platform,
            start_date=start_date,
            end_date=end_date,
            countries=region_countries,
            ad_types=",".join(AD_TYPES_VIDEO),
            networks=",".join(NETWORKS),
        )
        save_json(app_id, data, json_dir=json_dir, suffix=region)
        if save_xlsx_too:
            save_xlsx(app_id, data, xlsx_dir=xlsx_dir, product_name=pname, suffix=region)
        return (app_id, region, None)
    except Exception as e:
        return (app_id, region, e)


def run(
    app_list,
    start_date=None,
    end_date=None,
    os_platform="unified",
    countries=None,
    save_xlsx_too=False,
    year=None,
    week_tag=None,
    product_type="strategy_old",
    strict=False,
    concurrency=None,
):
    """
    app_list: list of (app_id, product_name)ã€‚
    API: GET /v1/{os}/ad_intel/creativesã€‚æ¯ä¸ªäº§å“è¯·æ±‚ 4 æ¬¡ï¼Œåˆ†åˆ«å¯¹åº” 4 ä¸ªå¸‚åœºï¼ˆäºšæ´²T1ã€æ¬§ç¾T1ã€T2ã€T3ï¼‰ï¼Œ
    åªæ‹‰ video ç±»å‹ã€å…¨éƒ¨æŒ‡å®š networksï¼›ç»“æœæŒ‰å¸‚åœºå­˜ä¸º {app_id}_äºšæ´²T1.json ç­‰ã€‚
    year, week_tag è‹¥éƒ½æä¾›ï¼Œåˆ™å†™å…¥ advertisements/{year}/{week_tag}/{product_type}/ ä¸‹ï¼ˆé»˜è®¤ä»… jsonï¼Œä¸å†™ xlsxï¼‰ã€‚
    strict=Falseï¼šå•ä¸ªè¯·æ±‚å¤±è´¥æ—¶è·³è¿‡å¹¶ç»§ç»­ï¼›strict=True æ—¶ä»»ä¸€å¤±è´¥å³æŠ›é”™ã€‚
    concurrencyï¼šå¹¶å‘æ•°ï¼Œé»˜è®¤ 1ï¼ˆä¸²è¡Œï¼‰ã€‚
    """
    token = load_token()
    use_advertisements = year is not None and week_tag is not None
    market_countries = load_market_countries()
    workers = int(concurrency) if concurrency is not None else DEFAULT_CONCURRENCY
    workers = max(1, min(workers, 20))

    tasks = []
    for app_id, product_name in app_list:
        app_id = (app_id or "").strip()
        if not app_id:
            continue
        pname = (product_name or app_id).strip() or app_id
        for region in REGIONS:
            if market_countries.get(region):
                tasks.append((app_id, pname, region))

    ok, fail = 0, 0
    if workers <= 1:
        session = get_session(token)
        for app_id, pname, region in tasks:
            print(f"  ğŸ”¹ æ‹‰å–åˆ›æ„æ•°æ®ï¼šapp_id={app_id} äº§å“={pname} åœ°åŒº={region}")
            json_dir = xlsx_dir = None
            if use_advertisements and year and week_tag:
                json_dir, xlsx_dir = get_output_dirs(year, week_tag, product_type, app_id, pname)
            region_countries = market_countries.get(region)
            try:
                data = fetch_ad_creatives_with_retry(
                    session, app_id, os_platform=os_platform,
                    start_date=start_date, end_date=end_date,
                    countries=region_countries,
                    ad_types=",".join(AD_TYPES_VIDEO),
                    networks=",".join(NETWORKS),
                )
                save_json(app_id, data, json_dir=json_dir, suffix=region)
                if save_xlsx_too:
                    save_xlsx(app_id, data, xlsx_dir=xlsx_dir, product_name=pname, suffix=region)
                ok += 1
            except Exception as e:
                if fail < MAX_SKIP_LOGS:
                    print(f"  âš ï¸ è·³è¿‡ {app_id} {region}: {e}")
                fail += 1
                if strict:
                    raise
    else:
        print(f"  åˆ›æ„æ•°æ®å¹¶å‘æ•°: {workers}")
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(
                    _fetch_one_region,
                    app_id, pname, region, token,
                    start_date, end_date, os_platform, market_countries,
                    use_advertisements, year, week_tag, product_type, save_xlsx_too,
                ): (app_id, region)
                for app_id, pname, region in tasks
            }
            for fut in as_completed(futures):
                app_id, region, exc = fut.result()
                if exc is None:
                    ok += 1
                else:
                    if fail < MAX_SKIP_LOGS:
                        print(f"  âš ï¸ è·³è¿‡ {app_id} {region}: {exc}")
                    fail += 1
                    if strict:
                        raise exc
    if fail:
        if fail > MAX_SKIP_LOGS:
            print(f"  ... å…¶ä½™ {fail - MAX_SKIP_LOGS} æ¬¡å·²è·³è¿‡")
        print(f"  åˆ›æ„æ•°æ®ï¼šæˆåŠŸ {ok}ï¼Œè·³è¿‡ {fail}")


def parse_app_list(app_ids=None, app_ids_file=None, app_list_file=None):
    """
    è§£æä¸º [(app_id, product_name), ...]ã€‚
    --app_ids / --app_ids_fileï¼šä»… app_idï¼Œproduct_name ç”¨ app_idï¼›
    --app_list_fileï¼šæ¯è¡Œ app_id,äº§å“å æˆ– app_id\\täº§å“åï¼Œé€—å·/åˆ¶è¡¨ç¬¦åˆ†éš”ã€‚
    """
    out = []
    if app_list_file and app_list_file.exists():
        for line in app_list_file.read_text(encoding="utf-8").strip().splitlines():
            line = line.strip()
            if not line:
                continue
            parts = re.split(r"[\t,]", line, maxsplit=1)
            app_id = (parts[0] or "").strip()
            product_name = (parts[1] or "").strip() if len(parts) > 1 else app_id
            if app_id:
                out.append((app_id, product_name))
    if app_ids:
        for a in app_ids:
            a = (a or "").strip()
            if a:
                out.append((a, a))
    if app_ids_file and app_ids_file.exists():
        for line in app_ids_file.read_text(encoding="utf-8").strip().splitlines():
            a = (line or "").strip()
            if a:
                out.append((a, a))
    return out


def main():
    parser = argparse.ArgumentParser(
        description="æ‹‰å–å¹¿å‘Šåˆ›æ„æ•°æ®ï¼Œå†™å…¥ advertisements/{å¹´}/{å‘¨}/ æˆ– request/ad_creatives/"
    )
    parser.add_argument("--app_ids", nargs="+", help="ä¸€ä¸ªæˆ–å¤šä¸ª app_idï¼ˆäº§å“åç”¨ app_idï¼‰")
    parser.add_argument("--app_ids_file", type=Path, help="æ¯è¡Œä¸€ä¸ª app_id çš„æ–‡ä»¶")
    parser.add_argument(
        "--app_list_file",
        type=Path,
        help="æ¯è¡Œ app_id,äº§å“å æˆ– app_id\\täº§å“åï¼Œç”¨äº advertisements ä¸‹æ–‡ä»¶å¤¹å",
    )
    parser.add_argument("--year", type=int, help="å¹´ä»½ï¼Œä¸ --week ä¸€èµ·æŒ‡å®šæ—¶å†™å…¥ advertisements/{å¹´}/{å‘¨}/")
    parser.add_argument("--week", dest="week_tag", help="å‘¨æ ‡ç­¾å¦‚ 0119-0125ï¼Œä¸ --year ä¸€èµ·æ—¶å†™å…¥ advertisements")
    parser.add_argument(
        "--product_type",
        default="strategy_old",
        help="äº§å“ç±»å‹å­ç›®å½•ï¼Œé»˜è®¤ strategy_oldï¼ˆstrategy_new ç­‰ï¼‰",
    )
    parser.add_argument("--os", dest="os_platform", default="unified", choices=["ios", "android", "unified"], help="å¹³å°ï¼ŒAPI è·¯å¾„ /v1/{os}/ad_intel/creativesï¼Œé»˜è®¤ unified")
    parser.add_argument("--countries", default="WW", help="å›½å®¶ä»£ç é€—å·åˆ†éš”ï¼Œé»˜è®¤ WWï¼ˆå…¨çƒï¼‰")
    parser.add_argument("--start_date", help="å¼€å§‹æ—¥æœŸ YYYY-MM-DDï¼ˆAPI å¿…å¡«å»ºè®®ï¼‰")
    parser.add_argument("--end_date", help="ç»“æŸæ—¥æœŸ YYYY-MM-DD")
    parser.add_argument("--xlsx", action="store_true", help="åŒæ—¶å†™å…¥ xlsxï¼ˆé»˜è®¤ä»…å†™ jsonï¼Œä¾› MySQL åŒæ­¥ï¼‰")
    parser.add_argument("--strict", action="store_true", help="ä»»ä¸€è¯·æ±‚å¤±è´¥å³é€€å‡ºï¼Œé»˜è®¤è·³è¿‡å¤±è´¥ç»§ç»­")
    parser.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, help=f"å¹¶å‘è¯·æ±‚æ•°ï¼Œé»˜è®¤ {DEFAULT_CONCURRENCY}ï¼›è®¾ä¸º 1 åˆ™ä¸²è¡Œ")
    args = parser.parse_args()

    app_list = parse_app_list(
        app_ids=args.app_ids,
        app_ids_file=args.app_ids_file,
        app_list_file=args.app_list_file,
    )
    if not app_list:
        print("è¯·æä¾› --app_idsã€--app_ids_file æˆ– --app_list_fileï¼ˆå« app_id[,äº§å“å]ï¼‰")
        sys.exit(1)

    # æœ‰ --year å’Œ --week æ—¶è‡ªåŠ¨è¡¥å…¨ start_date / end_dateï¼Œç¡®ä¿åˆ›æ„ API å§‹ç»ˆå¸¦ end_date
    start_date = args.start_date
    end_date = args.end_date
    if args.year and args.week_tag:
        derived_start, derived_end = week_tag_to_dates(int(args.year), args.week_tag)
        if derived_start and not start_date:
            start_date = derived_start
        if derived_end and not end_date:
            end_date = derived_end
    if start_date and not end_date:
        try:
            from datetime import datetime, timedelta
            dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_date = (dt + timedelta(days=6)).strftime("%Y-%m-%d")
        except Exception:
            pass

    run(
        app_list,
        start_date=start_date,
        end_date=end_date,
        os_platform=args.os_platform,
        countries=args.countries,
        save_xlsx_too=args.xlsx,
        year=args.year,
        week_tag=args.week_tag,
        product_type=args.product_type or "strategy_old",
        strict=args.strict,
        concurrency=args.concurrency,
    )


if __name__ == "__main__":
    main()
