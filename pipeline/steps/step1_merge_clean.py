import pandas as pd
from pathlib import Path
import argparse

# =================================================
# Sensor Tower CSV è¯»å–å‡½æ•°
# =================================================
def read_sensor_tower_csv(path: Path) -> pd.DataFrame:
    try:
        return pd.read_csv(
            path,
            encoding="utf-16",
            sep="\t",
            engine="c",
            low_memory=False,
        )
    except Exception:
        return pd.read_csv(
            path,
            encoding="utf-16",
            sep="\t",
            engine="python",
        )

# =================================================
# STEP1 ä¸»æµç¨‹
# =================================================
def run_step1(week_tag: str, year: int = None, write_normalized: bool = True):

    # === é¡¹ç›®æ ¹ç›®å½• SLG Monitor ===
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    # === è‡ªåŠ¨æ£€æµ‹å¹´ä»½ï¼ˆå¦‚æœæœªæä¾›ï¼‰ ===
    if year is None:
        # å°è¯•ä»week_tagæ¨æ–­å¹´ä»½ï¼Œæˆ–æŸ¥æ‰¾å­˜åœ¨çš„å¹´ä»½æ–‡ä»¶å¤¹
        week_year = None
        if week_tag and len(week_tag) >= 7:
            # å¦‚æœweek_tagæ˜¯1229-0104è¿™ç§è·¨å¹´æ ¼å¼ï¼Œéœ€è¦åˆ¤æ–­
            # ç®€å•ç­–ç•¥ï¼šæŸ¥æ‰¾æ‰€æœ‰å­˜åœ¨çš„å¹´ä»½æ–‡ä»¶å¤¹
            for y in range(2020, 2030):
                year_dir = BASE_DIR / f"{y}_raw_csv"
                if year_dir.exists():
                    week_dir = year_dir / week_tag
                    if week_dir.exists() and week_dir.is_dir():
                        week_year = y
                        break
        
        if week_year is None:
            # é»˜è®¤ä½¿ç”¨å½“å‰å¹´ä»½ï¼Œæˆ–æŸ¥æ‰¾æœ€æ–°çš„å¹´ä»½æ–‡ä»¶å¤¹
            from datetime import datetime
            current_year = datetime.now().year
            for y in range(current_year, current_year - 5, -1):
                year_dir = BASE_DIR / f"{y}_raw_csv"
                if year_dir.exists():
                    week_year = y
                    break
        
        if week_year is None:
            raise ValueError(f"âŒ æ— æ³•è‡ªåŠ¨æ£€æµ‹å¹´ä»½ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š --year å‚æ•°")
        
        year = week_year
        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°å¹´ä»½: {year}")

    # === åŸå§‹ CSV æ‰€åœ¨ç›®å½• {year}_raw_csv/0105-0111 ===
    RAW_DIR = BASE_DIR / f"{year}_raw_csv" / week_tag

    if not RAW_DIR.exists():
        try:
            from app.app_paths import get_data_root
            data_root = get_data_root()
            alt_dir = data_root / "raw_csv" / str(year) / week_tag
            if alt_dir.exists():
                RAW_DIR = alt_dir
        except Exception:
            pass

    if not RAW_DIR.exists():
        raise ValueError(f"âŒ æœªæ‰¾åˆ°ç›®å½•: {RAW_DIR}")

    # === æ ‡å‡†åŒ–å CSV è¾“å‡ºç›®å½• ===
    NORMALIZED_DIR = RAW_DIR / "normalized"

    # === STEP1 æœ€ç»ˆåˆå¹¶è¾“å‡ºç›®å½•ï¼šintermediate/{year}/{week_tag}/ ===
    OUTPUT_DIR = BASE_DIR / "intermediate" / str(year) / week_tag

    # âœ… å…³é”®ä¿®æ­£ï¼šå…è®¸è‡ªåŠ¨åˆ›å»ºæ‰€æœ‰çˆ¶ç›®å½•
    if write_normalized:
        NORMALIZED_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    FINAL_OUTPUT_PATH = OUTPUT_DIR / "merged_deduplicated.xlsx"

    # =================================================
    # 1. æ‰¾å‡ºæœ¬å‘¨ CSV
    # =================================================
    csv_files = sorted(RAW_DIR.glob(f"{week_tag}-*.csv"))

    print(f"\nğŸ“‚ æ£€æµ‹åˆ° {len(csv_files)} ä¸ªåŸå§‹ CSV")

    if len(csv_files) == 0:
        raise ValueError(f"âŒ æœªæ‰¾åˆ° {week_tag}-*.csv æ–‡ä»¶")

    # =================================================
    # 2. æ ‡å‡†åŒ– CSV (utf-16/tab â†’ utf-8)
    # =================================================
    df_list = []

    print("\nğŸ”¹ Step 1.1: è¯»å– CSV å¹¶åˆå¹¶")
    for f in csv_files:
        print(f"è¯»å–: {f.name}")
        df = read_sensor_tower_csv(f)
        if write_normalized:
            out_path = NORMALIZED_DIR / f.name
            df.to_csv(out_path, index=False, encoding="utf-8-sig")
            print(f"  âœ… è¾“å‡º: normalized/{out_path.name}")
        df_list.append(df)

    merged_df = pd.concat(df_list, ignore_index=True)

    print(f"   åˆå¹¶åæ€»è¡Œæ•°: {len(merged_df)}")

    # =================================================
    # 4. æŸ¥æ‰¾ Unified ID åˆ—
    # =================================================
    unified_id_candidates = ["Unified ID", "Unified_ID", "unified_id"]
    unified_col = next((c for c in unified_id_candidates if c in merged_df.columns), None)

    if unified_col is None:
        raise ValueError("âŒ æœªæ‰¾åˆ° Unified ID åˆ—")

    print(f"ğŸ”¹ ä½¿ç”¨å»é‡åˆ—: {unified_col}")

    # =================================================
    # 5. Step A: æŒ‰ Unified ID å»é‡ï¼ˆä»…å¯¹éç©º Unified ID ç”Ÿæ•ˆï¼‰
    # =================================================
    before = len(merged_df)
    uid_series = merged_df[unified_col].astype(str).str.strip()
    has_uid = uid_series.notna() & (uid_series != "") & (uid_series.str.lower() != "nan")
    df_with_uid = merged_df[has_uid].copy()
    df_no_uid = merged_df[~has_uid].copy()
    df_with_uid = df_with_uid.drop_duplicates(subset=[unified_col], keep="first")
    df_uid = pd.concat([df_with_uid, df_no_uid], ignore_index=True)
    after = len(df_uid)

    print("\nğŸ”¹ Step 1.3: Unified ID å»é‡")
    print(f"   å»é‡å‰è¡Œæ•°: {before}")
    print(f"   å»é‡åè¡Œæ•°: {after}")
    print(f"   åˆ é™¤è¡Œæ•°:   {before - after}")

    # =================================================
    # 6. Step B: Excel-like Revenue + Name å»é‡é€»è¾‘
    # =================================================
    revenue_col = "Revenue (Absolute)"
    name_col = "Unified Name"

    missing_cols = [c for c in [revenue_col, name_col] if c not in df_uid.columns]
    if missing_cols:
        raise ValueError(f"âŒ ç¼ºå°‘åˆ—ï¼Œæ— æ³•æ‰§è¡Œ Step B: {missing_cols}")

    before = len(df_uid)

    # B1: Revenue æ˜¯å¦é‡å¤æ‹†åˆ†
    rev_dup_mask = df_uid.duplicated(subset=[revenue_col], keep=False)
    df_rev_dup = df_uid[rev_dup_mask]
    df_rev_unique = df_uid[~rev_dup_mask]

    print("\nğŸ”¹ Step 1.4: Revenue é‡å¤æ€§æ‹†åˆ†")
    print(f"   Revenue ä¸é‡å¤è¡Œæ•°: {len(df_rev_unique)}")
    print(f"   Revenue é‡å¤è¡Œæ•°:   {len(df_rev_dup)}")

    # B2: é‡å¤éƒ¨åˆ† â†’ æŒ‰ Revenue é™åº â†’ Unified Name å¿½ç•¥å¤§å°å†™å»é‡
    df_rev_dup_sorted = df_rev_dup.sort_values(by=revenue_col, ascending=False)
    name_lower = df_rev_dup_sorted[name_col].astype(str).str.lower()
    df_rev_dup_dedup = df_rev_dup_sorted.loc[
        ~name_lower.duplicated(keep="first")
    ]

    print("\nğŸ”¹ Step 1.5: Unified Name å¿½ç•¥å¤§å°å†™å»é‡")
    print(f"   å»é‡å‰(é‡å¤éƒ¨åˆ†): {len(df_rev_dup)}")
    print(f"   å»é‡å(é‡å¤éƒ¨åˆ†): {len(df_rev_dup_dedup)}")
    print(f"   åˆ é™¤: {len(df_rev_dup) - len(df_rev_dup_dedup)}")

    # B3: åˆå¹¶å›æœ€ç»ˆç»“æœ
    df_final = pd.concat([df_rev_unique, df_rev_dup_dedup], ignore_index=True)

    after = len(df_final)

    print("\nğŸ”¹ Step 1.6: æœ€ç»ˆåˆå¹¶ç»“æœ")
    print(f"   StepB å»é‡å‰è¡Œæ•°: {before}")
    print(f"   StepB å»é‡åè¡Œæ•°: {after}")
    print(f"   StepB åˆ é™¤è¡Œæ•°:   {before - after}")

    # =================================================
    # 7. è¾“å‡ºæœ€ç»ˆ XLSX
    # =================================================
    df_final.to_excel(FINAL_OUTPUT_PATH, index=False)

    print("\n==============================")
    print("âœ… STEP 1 å…¨æµç¨‹å®Œæˆ")
    print(f"   æœ€ç»ˆå‰©ä½™è¡Œæ•°: {len(df_final)}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {FINAL_OUTPUT_PATH}")
    print("==============================\n")


# =================================================
# å‘½ä»¤è¡Œå…¥å£
# =================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--week", required=True, help="ä¾‹å¦‚ 0105-0111")
    parser.add_argument("--year", type=int, help="å¹´ä»½ï¼Œä¾‹å¦‚ 2025ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰")
    parser.add_argument("--no-normalize", action="store_true", help="ä¸è¾“å‡º normalized ç›®å½•ï¼ˆæ›´å¿«ï¼‰")
    args = parser.parse_args()

    run_step1(args.week, args.year, write_normalized=not args.no_normalize)
